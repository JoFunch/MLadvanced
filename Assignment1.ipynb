{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3c34883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import matplotlib.path as mplpath\n",
    "import torch.utils.data as data_utils\n",
    "import matplotlib.patches as patches\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a04d0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3377a27",
   "metadata": {},
   "source": [
    "## DATA PREP 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7a29c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(file_dir_path):\n",
    "    \n",
    "    with open(file_dir_path) as j_file:\n",
    "        list_of_marks = [] \n",
    "        json_list = list(j_file)\n",
    "        p = 0 \n",
    "        final = {}\n",
    "        for json_str in json_list:\n",
    "            if p > 50: #for relavative batch, for testing\n",
    "                return final #, list_of_marks\n",
    "            img = json.loads(json_str)\n",
    "            img_id = img['image_id']+'.jpg'\n",
    "            l_poly = []\n",
    "            \n",
    "            for key, value in img.items():\n",
    "                for v in img['annotations']: #all annotations per image\n",
    "\n",
    "                    for dic in v:\n",
    "                        if dic['is_chinese']:\n",
    "                           # print(dic['is_chinese'])\n",
    "                            coords = dic['polygon']\n",
    "                            l_poly.append(coords)\n",
    "                        \n",
    " #                           text = dic['text'] #for specific rather than 1/0 inside BB\n",
    "#                            list_of_marks.append(text) #for specific mark \n",
    "\n",
    "                              \n",
    "            p += 1\n",
    "            final[img_id] = l_poly\n",
    "                    \n",
    "    return final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0127166",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "poly_box = data('a1/train.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a37ef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_box(poly_boxes):\n",
    "    tp = []\n",
    "    for img_id,boxes in poly_boxes.items():\n",
    "        print('New Image for processing ... ')\n",
    "        img = cv2.imread('a1/images/'+img_id)\n",
    "        img = np.resize(img, (3 , 2048, 2048)) #2\n",
    "#        img = np.resize(img, (2048*2048,2))\n",
    "        grid = np.array([[[a,b] for b in list(range(2048))] for a in list(range(2048))])\n",
    "        grid = grid.reshape(2048*2048,2) \n",
    "        collected_poly = np.zeros(grid.shape[0])\n",
    "        for box in boxes:\n",
    "            pnts= np.array(box)\n",
    "            path = mplpath.Path(pnts)\n",
    "            new_new = path.contains_points(grid)\n",
    "            new_new = np.array(new_new, int)\n",
    "            collected_poly = np.maximum(collected_poly, new_new)\n",
    "        img = img/255\n",
    "        tp.append([img, collected_poly])\n",
    "    \n",
    "        \n",
    "\n",
    "    return tp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b829bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "086e6d1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n",
      "New Image for processing ... \n"
     ]
    }
   ],
   "source": [
    "mkb = make_box(poly_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8412b540",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = mkb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e5d79b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Train, test, val\n",
    "test_size = int(len(dataset)*0.2)\n",
    "\n",
    "val_size = int(len(dataset)*0.1)\n",
    "train_size = len(dataset) - (test_size+val_size)\n",
    "\n",
    "train_d = dataset[:train_size] # 07\n",
    "val_d = dataset[train_size:-test_size] #01\n",
    "test_d = dataset[(train_size+val_size):] # 02 \n",
    "\n",
    "#Train\n",
    "trainloader = torch.utils.data.DataLoader(train_d, batch_size=5,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "#Test\n",
    "testloader = torch.utils.data.DataLoader(test_d, batch_size=5,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0570c607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcb76fba",
   "metadata": {},
   "source": [
    "## MODEL 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7f00dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChineseChar(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ChineseChar, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 3, 5) \n",
    "        self.conv2 = nn.Conv2d(3, 16, 5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(4145296, 253)  # for 1 dim: 16*253*253, 120\n",
    "        self.fc2 = nn.Linear(253, 120) #120, 84\n",
    "        self.fc3 = nn.Linear(120, 64) #84,16*256*256 eller 84,2  # 60 #4194304\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8be2342",
   "metadata": {},
   "outputs": [],
   "source": [
    "Char = ChineseChar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83666417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecc46cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(Char.parameters(), lr=0.005, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d5bb1e",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "121c8800",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.5863032937049866\n",
      "Epoch [2/10], Loss: 0.21119970083236694\n",
      "Epoch [3/10], Loss: 0.048833709210157394\n",
      "Epoch [4/10], Loss: 0.02527598664164543\n",
      "Epoch [5/10], Loss: 0.04726000502705574\n",
      "Epoch [6/10], Loss: 0.030520491302013397\n",
      "Epoch [7/10], Loss: 0.019646162167191505\n",
      "Epoch [8/10], Loss: 0.02794039621949196\n",
      "Epoch [9/10], Loss: 0.009800591506063938\n",
      "Epoch [10/10], Loss: 0.024047497659921646\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 10\n",
    "for epoch in range(10):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data \n",
    "        inputs = inputs.to(torch.float)\n",
    "        labels = labels.to(torch.float)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        labels = torch.flatten(labels, 1)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = Char(inputs)\n",
    "\n",
    "        #print(labels.shape)\n",
    "        upsampling_alg = nn.Upsample(size=2048*2048)\n",
    "        outputs = upsampling_alg(outputs.unsqueeze(1))\n",
    "        enhanced_img = upsampling_alg(outputs.reshape(len(inputs), 1, 2048*2048))\n",
    "        \n",
    "        enhanced_img = enhanced_img.squeeze(1)\n",
    "        \n",
    "        \n",
    "        #outputs = predicted.squeeze(-1) #if output in BCE set to < len-size of flatten array      \n",
    "        \n",
    "\n",
    "        #labels = torch.max(labels, 1)[1].long() # if more than 2 outputs in model, enable here #0 for 0's prints none\n",
    "#        loss = criterion(outputs, labels)\n",
    "        loss = criterion(enhanced_img, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch [{}/{}], Loss: {}'.format(epoch+1, num_epoch, loss.item()))\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d382fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "#PATH = 'ModelType1'\n",
    "#torch.save(Char.state_dict(), PATH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91b28ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load\n",
    "#PATH = 'ModelType1'\n",
    "#Char = ChineseChar()\n",
    "#Char.load_state_dict(torch.load(PATH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1baeb5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.33\n",
      "Loss:  0.05392131209373474\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=np. inf)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "Char.eval()\n",
    "running_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(testloader):\n",
    "        \n",
    "        imgs, labels = data\n",
    "                \n",
    "        imgs = imgs.to(torch.float)\n",
    "        labels = labels.to(torch.float)\n",
    "        labels = torch.flatten(labels, 1)\n",
    "        \n",
    "        outputs = Char(imgs)\n",
    "        upsampling_alg = nn.Upsample(size=2048*2048)\n",
    "        enhanced = upsampling_alg(outputs.unsqueeze(1))\n",
    "\n",
    "        loss = criterion(enhanced.squeeze(), labels)\n",
    "        running_loss = loss.item()\n",
    "        total += labels.shape[0]\n",
    "        \n",
    "        correct += torch.sum(torch.eq(torch.argmax(enhanced, dim=1), labels))\n",
    "        accuracy = int(correct) / ((i+1) * 5 * 3*2048*2048)\n",
    "\n",
    "print('Accuracy:', round(accuracy,2))\n",
    "print('Loss: ', running_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2bd956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a19f8cb",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "564f0596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7ffc74ab20>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "r_img = random.choice(val_d)\n",
    "p_img = torch.tensor(r_img[0]).permute(2,1,0) #0 for img, 1 for label\n",
    "random_img = p_img.unsqueeze(0).permute(0,3,2,1).float() #to fake a batch-size for the model\n",
    "\n",
    "with torch.no_grad():\n",
    "    single_out = Char(random_img)\n",
    "\n",
    "upsampling_alg = nn.Upsample(size=2048*2048) #original image size\n",
    "enhanced_image = upsampling_alg(single_out.reshape(1,1,64)) #1 batch, 1 channel, img*img long\n",
    "enhanced_img = enhanced_image.reshape(2048,2048) #Collect into colour-arrays\n",
    "enhanced_img = enhanced_img.squeeze() #squeeze batch\n",
    "plt.imshow(p_img)\n",
    "plt.imshow(enhanced_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba78796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb5b832",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
